{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3,92,1) (3,3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 296\u001b[0m\n\u001b[0;32m    293\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mCategoricalCrossentropyLoss())\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 237\u001b[0m, in \u001b[0;36mSequential.fit\u001b[1;34m(self, X_train, y_train, epochs, batch_size, validation_data)\u001b[0m\n\u001b[0;32m    235\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m X_train[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m    236\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_train[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m--> 237\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    239\u001b[0m epoch_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m batch_size)\n",
      "Cell \u001b[1;32mIn[2], line 222\u001b[0m, in \u001b[0;36mSequential.train_step\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y, logits)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 209\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 209\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m, in \u001b[0;36mConv2D.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(conv_output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(conv_output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m---> 88\u001b[0m             conv_output[i, j, k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m:\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\n\u001b[0;32m     89\u001b[0m \u001b[43m                                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias[k]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m relu(conv_output)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3,92,1) (3,3,1) "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Define derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Define derivative of ReLU\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Define paths to train and test folders\n",
    "train_folder = r'C:\\Users\\diyam\\Documents\\datase\\Train'\n",
    "test_folder = r'C:\\Users\\diyam\\Documents\\mrlEyes_2018_01\\Prepared_Data\\Test'\n",
    "\n",
    "classes = ['open', 'closed']\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "img_size = (92, 112)\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_path = os.path.join(train_folder, class_name)\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
    "        X_train.append(img)\n",
    "        y_train.append(i)\n",
    "\n",
    "    class_path = os.path.join(test_folder, class_name)\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n",
    "        X_test.append(img)\n",
    "        y_test.append(i)\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.uint8)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test, dtype=np.uint8)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = np.eye(2)[y_train]\n",
    "y_test = np.eye(2)[y_test]\n",
    "\n",
    "learning_rate = 0.001  # Define the learning rate\n",
    "\n",
    "# Update Conv2D layer to correctly handle input channel dimension\n",
    "class Conv2D:\n",
    "    def __init__(self, filters, kernel_size, activation='relu'):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.weights = None\n",
    "        self.bias = np.zeros(filters)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if self.weights is None:\n",
    "            input_channels = inputs.shape[-1]\n",
    "            # Adjust the initialization of weights to match input_channels\n",
    "            self.weights = np.random.randn(self.kernel_size[0], self.kernel_size[1], input_channels, self.filters)\n",
    "        # Perform the convolution operation using numpy\n",
    "        conv_output = np.zeros((inputs.shape[0] - self.kernel_size[0] + 1,\n",
    "                                inputs.shape[1] - self.kernel_size[1] + 1,\n",
    "                                self.filters))\n",
    "        for k in range(self.filters):\n",
    "            for i in range(conv_output.shape[0]):\n",
    "                for j in range(conv_output.shape[1]):\n",
    "                    # Adjust the broadcasting of inputs and weights\n",
    "                    conv_output[i, j, k] = np.sum(inputs[i:i+self.kernel_size[0], j:j+self.kernel_size[1], :] *\n",
    "                                                   self.weights[:, :, :, k]) + self.bias[k]\n",
    "        if self.activation == 'relu':\n",
    "            return relu(conv_output)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return sigmoid(conv_output)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_input = np.zeros_like(self.inputs)\n",
    "        d_weights = np.zeros_like(self.weights)\n",
    "        d_bias = np.zeros_like(self.bias)\n",
    "        if self.activation == 'relu':\n",
    "            activation_derivative = relu_derivative\n",
    "        elif self.activation == 'sigmoid':\n",
    "            activation_derivative = sigmoid_derivative\n",
    "        for k in range(self.filters):\n",
    "            for i in range(d_out.shape[0]):\n",
    "                for j in range(d_out.shape[1]):\n",
    "                    d_input[i:i+self.kernel_size[0], j:j+self.kernel_size[1], :] += d_out[i, j, k] * self.weights[:, :, :, k]\n",
    "                    d_weights[:, :, :, k] += d_out[i, j, k] * self.inputs[i:i+self.kernel_size[0], j:j+self.kernel_size[1], :]\n",
    "                    d_bias[k] += d_out[i, j, k]\n",
    "        return d_input, d_weights, d_bias\n",
    "\n",
    "\n",
    "\n",
    "# Define custom MaxPooling2D Layer\n",
    "class MaxPooling2DLayer:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        pooled_output = np.zeros((inputs.shape[0] // self.pool_size[0],\n",
    "                                  inputs.shape[1] // self.pool_size[1],\n",
    "                                  inputs.shape[2]))\n",
    "        for i in range(0, inputs.shape[0], self.pool_size[0]):\n",
    "            for j in range(0, inputs.shape[1], self.pool_size[1]):\n",
    "                for k in range(inputs.shape[2]):\n",
    "                    pooled_output[i//self.pool_size[0], j//self.pool_size[1], k] = np.max(inputs[i:i+self.pool_size[0], j:j+self.pool_size[1], k])\n",
    "        return pooled_output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_input = np.zeros_like(self.inputs)\n",
    "        for i in range(d_out.shape[0]):\n",
    "            for j in range(d_out.shape[1]):\n",
    "                for k in range(self.inputs.shape[2]):\n",
    "                    window = self.inputs[i*self.pool_size[0]:i*self.pool_size[0]+self.pool_size[0],\n",
    "                                          j*self.pool_size[1]:j*self.pool_size[1]+self.pool_size[1], k]\n",
    "                    max_value = np.max(window)\n",
    "                    d_input[i*self.pool_size[0]:i*self.pool_size[0]+self.pool_size[0],\n",
    "                            j*self.pool_size[1]:j*self.pool_size[1]+self.pool_size[1], k] = (window == max_value) * d_out[i, j, k]\n",
    "        return d_input\n",
    "\n",
    "# Define custom Dropout Layer\n",
    "class DropoutLayer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.rand(*inputs.shape) < self.rate\n",
    "            return np.where(self.mask, 0, inputs)\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return np.where(self.mask, 0, d_out)\n",
    "\n",
    "# Define custom Flatten Layer\n",
    "class Flatten:\n",
    "    def forward(self, inputs):\n",
    "        self.input_shape = inputs.shape\n",
    "        return inputs.flatten().reshape((inputs.shape[0], -1))\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out.reshape(self.input_shape)\n",
    "\n",
    "# Define custom Dense Layer\n",
    "class Dense:\n",
    "    def __init__(self, units, activation='relu'):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(units)\n",
    "        self.bias = np.zeros(units)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        dense_output = np.dot(inputs, self.weights) + self.bias\n",
    "        if self.activation == 'relu':\n",
    "            return relu(dense_output)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return sigmoid(dense_output)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        d_weights = np.dot(self.inputs.T, d_out)\n",
    "        d_bias = np.sum(d_out, axis=0)\n",
    "        return d_input, d_weights, d_bias\n",
    "\n",
    "# Define custom Softmax Layer\n",
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def backward(self, y_true, y_pred):\n",
    "        batch_size = len(y_true)\n",
    "        d_loss = y_pred - y_true\n",
    "        return d_loss / batch_size\n",
    "\n",
    "# Define custom Sequential model\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_out = layer.backward(d_out)\n",
    "        return d_out\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.forward(inputs, training=False)\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "        # Forward pass\n",
    "        logits = self.forward(X)\n",
    "        loss = self.loss_fn(y, logits)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad = self.loss_fn.backward(y, logits)\n",
    "        self.backward(grad)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs, batch_size, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                X_batch = X_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "                loss = self.train_step(X_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            epoch_loss /= (len(X_train) / batch_size)\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}')\n",
    "            if validation_data:\n",
    "                val_loss = self.evaluate(validation_data[0], validation_data[1])\n",
    "                print(f'Validation Loss: {val_loss}')\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        loss = self.loss_fn(y_test, y_pred)\n",
    "        return loss\n",
    "\n",
    "    def compile(self, loss):\n",
    "        self.loss_fn = loss\n",
    "\n",
    "# Define custom Categorical Crossentropy Loss\n",
    "class CategoricalCrossentropyLoss:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # Avoid division by zero\n",
    "        epsilon = 1e-15\n",
    "        # Clip values to prevent NaNs in log\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        # Compute cross-entropy loss\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y_true, y_pred):\n",
    "        # Avoid division by zero\n",
    "        epsilon = 1e-15\n",
    "        # Clip values to prevent NaNs in log\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        # Compute gradient of loss with respect to y_pred\n",
    "        grad = -y_true / y_pred\n",
    "        return grad\n",
    "\n",
    "# Define custom Accuracy metric\n",
    "class Accuracy:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Create the model\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2DLayer(pool_size=(2, 2)),\n",
    "    DropoutLayer(rate=0.25),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2DLayer(pool_size=(2, 2)),\n",
    "    DropoutLayer(rate=0.25),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    DropoutLayer(rate=0.5),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=CategoricalCrossentropyLoss())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
